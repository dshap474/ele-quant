**Chapter 4**
**Linear Models of Returns**

**The Questions**

1.  What is a factor model for asset returns, and how is it formulated?
2.  What are the different interpretations of factor models (graphical model, superposition of effects, single-asset product)?
3.  How do alpha spanned and alpha orthogonal components contribute to asset returns?
4.  What are the different transformations applied to factor models (rotations, projections, push-outs), and how do they affect the models?
5.  What are the practical applications of factor models in finance, such as performance attribution, risk management, portfolio construction, and alpha research?
6.  What are the different types of factor models (characteristic, statistical, macroeconomic), and how are they used in practice?

Linear models of asset returns are a cornerstone of this book. They are flexible, interpretable, perform well in applications, and have supporting theory. For this reason, they fit like a glove with mean-variance optimization and can be used as a basis for a number of important tasks, like risk management and performance analysis. It is possible that you, the reader, will find this class of models inadequate in some way, at some point. But just because you have outgrown them does not mean that you will find them useless. This chapter enables later chapters about the entire investment process, and some of the theory we use in later.

---

**4.1 Factor Models**

We saw in Chapter 2 how to model single-asset returns. It is more convenient to model returns as an independent process. This would not be adequate, however, because the returns are dependent. It is a natural step to model the common dependency among stocks as being generated by a few common sources of randomness, called factors, and then to keep a random shock per security that is independent of the factors. The model that describes returns in terms of common factors is called a factor model, and takes the form:
$$ r_{it} = \alpha_i + \mathbf{B}_i' \mathbf{f}_t + \epsilon_{it} $$
where
$i \in \{1, \dots, N\}$ denotes the discrete time period.
The random vector $\mathbf{f}_t$ denotes $K$ asset returns minus the risk-free rate (see Section 2.1.2).
The term $\alpha_i$ is a scalar, the idiosyncratic alpha vector.
The random vector $\mathbf{\epsilon}_t$ denotes $N$ factor returns. $N$ is much smaller than $K$ in most models. Interpret factor returns as pervasive sources of uncertainty in the market, affecting in some ways all asset returns.
$\mathbf{B}_i$ is an $N \times K$ loading matrix. The row-vector $\mathbf{B}_{i \cdot}$ is the loading of asset $i$ on the factors. The element $B_{ik}$ is the loading of asset $i$ on factor $k$. Interpret $\mathbf{B}_i$ as a matrix specifying how factor returns map to asset returns.
$\epsilon_{it}$ is the random vector of idiosyncratic returns.[1] You can interpret these returns in two ways. The positive interpretation is “returns that are specific to the asset and are uncorrelated with all other returns in the universe.” The negative interpretation is “component of returns that are left over after removing the pervasive sources of risk”.

If the random vector $\mathbf{f}_t$ had a generic distribution, we would gain nothing in tractability. Instead, we assume that (i) the vector $\mathbf{f}_t$ is independent of the factor returns $\epsilon_{it}$ at time $t$, (ii) $E[\mathbf{f}_t] = \mathbf{0}$, (iii) $E[\epsilon_{it}] = 0$, (iv) the $N \times N$ covariance matrix $\Omega_{ff} = E[\mathbf{f}_t \mathbf{f}_t']$ is diagonal, (v) the $N \times N$ covariance matrix $\Omega_{\epsilon\epsilon} = E[\mathbf{\epsilon}_t \mathbf{\epsilon}_t']$ is diagonal. Often, we assume that the idiosyncratic returns $\epsilon_{it}$ are uncorrelated across assets and serially uncorrelated. The idiosyncratic component of asset returns.

We usually refer to the term $\mathbf{B}_i' \mathbf{f}_t$ as the systematic component of asset returns. We assume that the pair $(\mathbf{f}_t, \mathbf{\epsilon}_t)$ is either identically distributed across periods or has a slowly varying distribution, and that $\mathbf{f}_t$ and $\mathbf{\epsilon}_t$ are independent for each $t$. We denote $E[\mathbf{f}_t \mathbf{f}_t'] = \Omega_f$ and $E[\mathbf{\epsilon}_t \mathbf{\epsilon}_t'] = \Omega_\epsilon$. Then, the $N \times N$ covariance matrix of assets is
(4.1)
$$ \Omega_r = \mathbf{B} \Omega_f \mathbf{B}' + \Omega_\epsilon $$
This decomposition is at the core of volatility modeling with linear returns and the

(Right Sidebar from Page 120)
subject of Chapters 5 and 7.

**FAQ 4.1: Why is the covariance matrix**
$$ \Omega_r = \mathbf{B} \Omega_f \mathbf{B}' + \Omega_\epsilon $$
?
The covariance matrix $\Omega_r$ does not depend on the intercept $\alpha_i$ and the terms $E[\mathbf{f}_t]$ and $E[\mathbf{\epsilon}_t]$ are independent, so that
$$ \text{cov}(\mathbf{Bf}_t, \mathbf{\epsilon}_t) = \text{cov}(\mathbf{Bf}_t) + \text{cov}(\mathbf{\epsilon}_t) = \mathbf{0} $$
The term $\mathbf{Bf}_t$ is a linear transformation of $\mathbf{f}_t$. For any random vector $\mathbf{z}$ with covariance $\Omega_z$. For any
$$ \text{cov}(\mathbf{Bz}) = \mathbf{B} \Omega_z \mathbf{B}' $$
, because
$$ \text{cov}(\mathbf{B}_{i \cdot} \mathbf{z}, \mathbf{B}_{j \cdot} \mathbf{z}) = E[(\mathbf{B}_{i \cdot} \mathbf{z} - E[\mathbf{B}_{i \cdot} \mathbf{z}]) (\mathbf{B}_{j \cdot} \mathbf{z} - E[\mathbf{B}_{j \cdot} \mathbf{z}])'] = \mathbf{B}_{i \cdot} \Omega_z \mathbf{B}_{j \cdot}' $$

In this chapter we set aside the very important issue of estimating the parameters of Equation (4.1). We take data, and focus instead on the usage and interpretation. There is a plan for the rest of the chapter. First, we review the interpretations of factor models, of which there are three:
1.  As a graphical model.
2.  As the superposition of low-dimensional cross-sectional returns vectors.
3.  As the overlap of the factor return vector with the asset loadings vector.

We then review the concepts of alpha, namely alpha spanned and alpha orthogonal. These are fundamental sources at the core of the research process. Third, we review the transformations that can be operated on factor models. There are three of those too:
1.  Rotations keep the dimension of the model unchanged and its predictions “invariant”.
2.  Projections reduce the dimension of the model.
3.  Push-outs increase the dimension of the model by adding factors.
These mathematical operations are versatile tools in the hands of the quantitative manager, to reformulate, simplify, or extend a model.

Fourth, we describe the uses of factor models. There are quite a few:
1.  Forecast and decompose volatility, so that we can separate wanted versus unwanted risk.
2.  Be a fundamental input to portfolio construction.
3.  Understand performance and separate skill from luck.
4.  Serve as a foundation for alpha research.

---

**4.2 Interpretations of Factor Models**

Before attempting to interpret factor models, let us make the factor model more concrete with an example. In Figure 4.1 we list a “typical” loading matrix used in a risk model. K, the number of columns contain style loadings. Other columns consist of dummy variables, indicating whether the stock belongs to a particular industry. There may be a column for “energy explorers and producers,” a column for “biotechnology companies,” and so on. A stock will have a “1” loading if it belongs to the industry, “0” otherwise. Finally, there are columns that consist of dummy variables indicating country classification, similarly to industry. When the factor return for a country or an industry is high, it will move all the stocks in the industry. The factor structure captures comovement among stocks with certain obvious commonalities, as well as less obvious ones.

[Image: Figure 4.1: A typical loadings matrix, partitioned into different blocks. The style loadings comprise an “intrinsic factor” (sometimes termed “fundamental factor”). The loadings of style and the industry and country classifications are the same for all stocks. The other style loadings are often standardized. The country and industry loadings take values equal to 1 if the asset belongs to the country or industry.]
(Table shows example factor loadings for assets: style (e.g., size, value, momentum), country (e.g., US, EU, JP), industry (e.g., tech, fin, healthcare))

Even though Equation (4.1) is older than modern statistics (having really originated in the unpublished work of Gauss), it is surprisingly rich in meaning, and becomes even richer when used in financial applications. First, let’s review some interpretations of the equation.

**4.2.1 Graphical Model**

The first one is as a graphical model.[2] Since $r_i - \alpha_i = \mathbf{B}_{i \cdot} \mathbf{f}$ for each asset $j$, this equation holds:
Each of the many asset returns is dependent on all, or some of, the few factor returns. In a typical regional risk model (say, America, Asia, or Europe) we have up to 10,000 assets and up to 100 factors. Figure 4.2 shows the relationship. The dependency of asset returns on factor returns is through the links provided by loadings $\mathbf{B}$ (arrows). When the matrix $\mathbf{B}$ is sparse, the corresponding graph is sparse.

[Image: Figure 4.2: Factor models as graphical models. Diagram shows Factor Returns at the top, connected by Loadings (arrows) to Asset Returns at the bottom.]

**4.2.2 Superposition of Effects**

The second interpretation is as an overlap of influences on asset returns. A model for the co-direction of returns, i.e., the vector of returns at a given point in time. Let $\mathbf{B}_{\cdot j}$ be the $j$-th column of the matrix $\mathbf{B}$. We rewrite $E(r - \alpha) = \mathbf{Bf}$ as
$$ E(r_i - \alpha_i | \mathbf{f}) = \sum_j B_{ij} f_j $$
The vector of expected excess returns is the superposition of a small number of vectors (the loadings $\mathbf{B}_{\cdot j}$ for a specific factor), weighted by the factor return. This makes it clear that the factor component of the cross-section lies in a low-dimensional space (the column subspace of $\mathbf{B}$). This is shown in Figure 4.3.

[Image: Figure 4.3: A factor model as the superposition of weighted factor loadings. Diagram shows Asset Returns (R_i,1, R_i,2) as a vector sum of f1 and f2, where f1 and f2 are scaled factor loadings.]

[Image: Figure 4.4: Factor models as scalar products of per-stock loadings and factor returns. Diagram shows a vector R_i,1 with projections onto factor vectors (f_1, B_i,1), (f_2, B_i,2), (f_3, B_i,3).]

**4.2.3 Single-Asset Product**

The last interpretation applies to single assets. The expected return of an asset given the factor returns is equal to the scalar product of the asset loadings and the vector of factor returns:
$$ E(r_i - \alpha_i | \mathbf{f}) = \langle \mathbf{B}_{i \cdot}, \mathbf{f} \rangle $$
While this formula is rarely used at the asset level, it does show up all the time when we apply it to portfolios. Let $w_i$ be the weight of asset $i$, where $w_i$ is the Asset Market Value invested in asset $i$; for instance, $w_i$ is the stock price times the number of shares held long or short. The expected PnL of the portfolio is
$$ E(\mathbf{w}' \mathbf{r} | \mathbf{f}) = E \left( \sum_i w_i (\alpha_i + \mathbf{B}_{i \cdot} \mathbf{f}) \right) $$
$$ = \sum_i w_i \alpha_i + \left( \sum_i w_i \mathbf{B}_{i \cdot} \right) \mathbf{f} $$
The relationship is illustrated in Figure 4.4.[3] In the special case of $K=1$, we still have an interpretation of the term $\langle \mathbf{B}_{i \cdot}, \mathbf{f} \rangle$. This is the PnL attributable to factor returns. We explain this PnL factor of a portfolio in terms of a scalar product, and within the scalar product identify the largest contributor, the degree of dispersion of PnL among the factors, and so on. This is the jumping-off point for performance attribution, which we will cover extensively later in the book.

---

**4.3 Alpha Spanned and Alpha Orthogonal**

Consider the factor equation
$$ \mathbf{r}_t = \alpha + \mathbf{B} \mathbf{f}_t + \mathbf{\epsilon}_t $$
where $\mathbf{f}_t$ and $\mathbf{\epsilon}_t$ are iid mean zero, and $\mathbf{f}_t$ and $\mathbf{\epsilon}_t$ are independent on $t=s$, with zero unconditional mean and finite variance. Decompose $\alpha$ as the sum of its projection on the column subspace of $\mathbf{B}$ (i.e., the image of the operator $\mathbf{B}$) and the orthogonal complement: $\alpha = \mathbf{B} \lambda + \alpha_\perp$. By construction, the equality $\mathbf{B}' \alpha_\perp = \mathbf{0}$ holds. So we have
$$ \mathbf{r}_t = \mathbf{B} \lambda + \alpha_\perp + \mathbf{B} \mathbf{f}_t + \mathbf{\epsilon}_t = \mathbf{B}(\lambda + \mathbf{f}_t) + \alpha_\perp + \mathbf{\epsilon}_t $$
In this relationship, you can see that there is an indeterminacy in the factor model. It is useful to rewrite the model as
$$ \alpha_\perp := \mathbf{B} \lambda + E[\mathbf{r}_t] - \mathbf{B} E[\mathbf{f}_t] - E[\mathbf{\epsilon}_t] + E[\mathbf{\epsilon}_t] $$
where
$$ \alpha_s := \mathbf{B} \lambda + E[\mathbf{r}_t] - E[\mathbf{\epsilon}_t] $$
The “alpha” spanned by the column subspace of $\mathbf{B}$ is indistinguishable from the expected returns of the factors. However, it shows that $\alpha$ can be split into the sum of two orthogonal (by construction) terms. In the remainder of the book, we will use $\alpha$ and $\mu_f = E[\mathbf{f}_t]$. Now, if you choose a portfolio proportional to the alpha vector
$$ \mathbf{w} = \frac{\alpha_\perp}{\|\alpha_\perp\|} $$
its payoff is
$$ \mathbf{w}' \mathbf{r}_t = \frac{1}{\|\alpha_\perp\|} \alpha_\perp' \mathbf{r}_t = \frac{1}{\|\alpha_\perp\|} \alpha_\perp' (\alpha_\perp + \mathbf{\epsilon}_t) = \|\alpha_\perp\| + \frac{\alpha_\perp' \mathbf{\epsilon}_t}{\|\alpha_\perp\|} $$
The expected return and variance of this portfolio are
$$ E(\mathbf{w}' \mathbf{r}_t) = \|\alpha_\perp\| $$
$$ \text{var}(\mathbf{w}' \mathbf{r}_t) = \frac{\alpha_\perp' \Omega_\epsilon \alpha_\perp}{\|\alpha_\perp\|^2} $$
There is an upper bound for the variance, given by the operator norm:
$$ \frac{\alpha_\perp' \Omega_\epsilon \alpha_\perp}{\|\alpha_\perp\|^2} \le \|\Omega_\epsilon\| $$
So that
$$ \text{SR} = \frac{E(\mathbf{w}' \mathbf{r}_t)}{\sqrt{\text{var}(\mathbf{w}' \mathbf{r}_t)}} \ge \frac{\|\alpha_\perp\|}{\sqrt{\|\Omega_\epsilon\|}} $$
In the case of a diagonal matrix, $\|\Omega_\epsilon\|$ is the largest element on the diagonal. Assume that it has an upper bound. This has interesting implications. Consider the case, for example, where the average absolute orthogonal return per asset is positive:
$$ \frac{1}{N} \sum_i |\alpha_{\perp i}| = \bar{\alpha}_\perp > 0, \text{ or, equivalently, } \|\alpha_\perp\|_1 = N \bar{\alpha}_\perp $$
Now, use the simple inequality between $L_1$ and Euclidean norm $\|\alpha_\perp\|_1 \le \sqrt{N} \|\alpha_\perp\|$. Hence $\|\alpha_\perp\| \ge \sqrt{N} \bar{\alpha}_\perp$. Apply this to the Sharpe Ratio of the alpha-orthogonal strategy, and we obtain a lower bound on the Sharpe Ratio:
(4.2)
$$ \text{SR} \ge \frac{\|\alpha_\perp\|}{\sqrt{\|\Omega_\epsilon\|}} \ge \sqrt{N} \frac{\bar{\alpha}_\perp}{\sqrt{\|\Omega_\epsilon\|}} $$
Let’s summarize the assumptions made so far, besides the fact that the factor model is correct. If we assume that:
1.  the largest idiosyncratic variance $\|\Omega_\epsilon\|$ is uniformly bounded in the number of assets, and
2.  the average absolute value of the coordinate of $\alpha_\perp$ is bounded below by $\bar{\alpha}_\perp$.

We have obtained a lower bound on the Sharpe strategy of a portfolio. And if these bounds are uniform for increasing values of $N$, then we have a sequence of Sharpe Ratios going to infinity! This is highly unlikely in real life, so that at least one of the assumptions we made—factor model, bound on idiosyncratic variances, bound on orthogonal expected returns—is likely incorrect. Under the assumption that the factor model is correct, and that the idiosyncratic variances are bounded, common fields in practice. This leaves us with the fact that the idiosyncratic orthogonal expected returns are vanishing as $N \to \infty$. Let us summarize this result in concrete terms:

*   If a linear model is a good approximation of returns, then alpha is either “spanned” or “orthogonal”.
*   Alpha spanned is extremely valuable. If you have positive alpha spanned, then your Sharpe Ratio increases at the rate $\sqrt{N}$, a typical rate that arises when you can diversify risk without giving up on returns.
*   There are excess returns, but they are more likely to come from “alpha spanned” than alpha, as we will see in the next chapter, sources with ties that does not diversify away with large number of assets.

---

**4.4 Transformations**

A factor model is not uniquely identified. Let $C$ be an $m \times m$ invertible matrix, and define
$$ \tilde{\mathbf{B}} = \mathbf{B} C^{-1} $$
$$ \tilde{\mathbf{f}} = C \mathbf{f} $$
The columns of $\tilde{\mathbf{B}}$ span the same subspace as the columns of $\mathbf{B}$. The model
$$ \mathbf{r} = \alpha + \tilde{\mathbf{B}} \tilde{\mathbf{f}} + \mathbf{\epsilon} $$
has the same returns as the original model. This is usually termed rotational indeterminacy of the factor model.
The covariance matrix of the transformed factors is $\tilde{\Omega}_f = C \Omega_f C'$. There will be several applications of rotational indeterminacy in the book. Rotations enable us to provide final users with different views of the same model. We explore the impact of factor covariance, orthonormal factors, and returns on three instructive examples: identity factor covariance, orthonormal loadings, and user’s needs.
Identity factor covariance matrices. Sometimes, users of a model would like to see uncorrelated factor returns with unit variance. Under this perspective, exposures to the portfolio can be interpreted directly as the squared volatilities, and the factor PnL of a portfolio is the sum of the squared exposures, without covariance terms.
This risk model perspective can be obtained by taking the Singular Value Decomposition (SVD) of $\Omega_f = U S U'$ and then setting $C = S^{-1/2} U'$. It follows that
$$ \tilde{\Omega}_f = C \Omega_f C' = S^{-1/2} U' U S U' U S^{-1/2} = I $$
Orthonormal loadings. We can also choose a representation so that the factor loadings are orthonormal $\tilde{\mathbf{B}}' \tilde{\mathbf{B}} = I_K$. This means that each column of $\tilde{\mathbf{B}}$ has unit norm (but not unit variance, since the columns may have non-zero means), and is orthogonal to the other. In this case the transformation is $C = V S^{1/2}$, where $V$ comes from the SVD $\mathbf{B}' \mathbf{B} = U S V'$. Then $\tilde{\mathbf{B}} = \mathbf{B} V S^{-1/2} U'$.
User’s needs. The choice of factor loadings is a common procedure. It consists of a linear rescaling of the loadings of one or more factors, so that the new loadings have zero mean and unit variance (once they are zero-mean, they also have unit squared norm). The benefit of this transformation is that it makes it easier to relate to the stock characteristics. What is the average exposure to the factor? How much? What is the average portfolio exposure to the factor on a standardized basis? Such a linear transformation resulting in an equivalent

(Right Sidebar from Page 135)
**FAQ 4.2: Is a scoring factor loadings changing a factor model?**
A scoring the loadings of a model will result in a model that makes the same predictions as the original one. The “new loadings” are a linear combination of the original factor loadings. For example, if the first factor is $f_1$, the second factor is $f_2$, ..., the $K$-th factor is $f_K$, a linear combination of the loadings vectors of the original model. A special case is the one where the “scoring factor” (or “derived factor”) is a scoring of the original factors. Even if the scoring factor results in a model that produces different risk forecasts and performance attributions.

factor model is possible by multiple the loadings of factor $j$ by constant $c_j$, and just consider
$$ C := \text{diag}(c_1, \dots, c_m, \dots, c_K) $$
which is always invertible. However, in general it is not possible to center the loadings (you can try to find a counterexample in Exercise 4.2). However, assume that the unit vector is in the subspace spanned by the loadings (i.e., there is a vector $\mathbf{v}$ such that $\mathbf{1}_N = \mathbf{Bv}$). In this case, the centering is possible, if we want to add constants $b_k$ to the loadings, then
$$ \tilde{\mathbf{B}} = \mathbf{B} E' E + (\mathbf{1}_N \mathbf{a}' - \mathbf{B} \mathbf{A}') \mathbf{B}' \mathbf{B} (\mathbf{v}) $$
, hence our transformation is
$$ C^{-1} = I_{m \times m} + \mathbf{A} \mathbf{a}' \mathbf{B} \text{ diag}(\mathbf{v}) $$
This assumption is verified in two common cases. The first one is the use of a “market” factor, defined as a linear combination of all asset returns. The second case is when there are country or industry factors, such that for each asset the sum of the loadings across industries is exactly one. In this case $a_k = B_{ik}$ for a vector that has ones in positions corresponding to the industry factors, and zero otherwise.

---

**4.4.2 Projections**

Occasionally, we want to use a risk model with fewer factors compared to the original one. At first glance, this operation may seem unjustified. If we have a better risk model in the model, why would we want to replace it with a different one? The reasons are many. For example, it may be the case that in practice the loadings of one or more factors are changing so fast as to make portfolio management from the hedge fund difficult. Another reason is that we are using a limited set of factors, perhaps because of limitations in the data available, or because we want to provide the end user with a “simplified” risk model that is as accurate as possible, while retaining the full model for other uses. For these reasons and more, we need to find a different risk model that is close, in some sense, to the original one.
We have a model $r = \alpha + Bf + \epsilon$ and associated covariance matrix
$$ \Omega_r = B \Omega_f B' + \Omega_\epsilon $$
...but we want to employ a different loadings matrix $\tilde{A}$, which the range of $\tilde{A}$ is contained in the range of $B$. If we model returns as $r = \alpha + \tilde{A}g + \eta$, the covariance model would be
$$ \Omega_r = \tilde{A} \Omega_g \tilde{A}' + \Omega_\eta $$
What is the model resulting in the best approximation to our original model? Let the range of $\tilde{A}$ be the subspace spanned by the first $k$ columns of $B$. Let $\tilde{B}$ denote the first $k$ columns of $B$. The optimal approximating factor returns are $g = B_k' f$, where
$$ \tilde{A} = (B_k' B_k)^{-1} B_k' B $$
The corresponding value is $\Omega_g = B_k' \Omega_f B_k$.

(Right Sidebar from Page 139)
**FAQ 4.3: Which projections?**
What types of projections are useful in practice?
1.  The columns of the matrix $\tilde{A}$ are a subset of the columns of the matrix $B$. In this case, we are attributing all the common risk and return to a restricted factor model derived from the original one. This is by far the main application of projections.
2.  The subspace spanned by the columns of $\tilde{A}$ is not contained in that spanned by the columns of $B$. In this case, the application is the qualitative ability of the second model to describe the factor risk predictions of the first model.

We call the operator $\Pi$ a projection because, like geometric projections, they are idempotent. An idempotent linear operator $\Pi$ is such that $\Pi^2 = \Pi$. The geometric meaning is that, if we project a vector onto a plane, projecting the result of the projection onto the same plane does not result in another vector, since the input vector is already on the plane.

**4.4.3 Push-Outs**

In the previous two sections we introduced a transformation that preserves the number of factors and a transformation that reduces it. This last section focuses on a transformation that increases the number of factors. We identify the $k$ columns of $B$ that are of interest, and we want to retain them as they are in the original one. Why could this be of interest? A possible scenario that occurs in practice is that our factor model may have been developed on historical data that are not representative of the current regime. As a result, the idiosyncratic returns show some systematicity that we believe they are amenable to be formulated as a different factor model.
$$ \epsilon = Ag + \eta $$
with $A \in \mathbb{R}^{N \times m}$, $g \in \mathbb{R}^m$ random variable (e.g. taking values in $\mathbb{R}^m$) and $\eta \in \mathbb{R}^N$ taking values in $\mathbb{R}^N$. The new model becomes
$$ r = \alpha + Bf + Ag + \eta $$
with $\eta$ uncorrelated from $f, g$. In the specification of the new model (4.3), we require that $A'B = 0$. If not, then some of the original factors would have to be re-defined. Assume that $B = [B_1, B_2]$, where we can write the original model as $r = \alpha + B_1 f_1 + B_2 f_2 + \epsilon$. We can then choose $A$ as the sum of parallel and orthogonal components. In matrix terms, $A = BC - B_1$. For some $C \in \mathbb{R}^{K \times m}$, it follows that the model $r = \alpha + B_1(f_1 - Cg) + B_2 f_2 + Ag + \eta$ is
$$ r = \alpha + B_1(f_1 + Cg) + B_2 f_2 + \eta $$
1.  $y_1' B_1 = 0$ (original model orthogonality condition)
2.  $y_2' B_2 = 0$ (residual model orthogonality condition)
3.  $(y_1' B_1 + Ag)' = 0$ (final model orthogonality condition)
from the second and third equalities it follows that $g'A'B_1 = 0$; the first equality can be rewritten as
$$ 0 = (g'A' + \eta') B_1 = g'A'B_1 $$
for all realizations of $f, g$, hence $A'B_1 = 0$. The example above assumed that the idiosyncratic returns of each asset have the same volatility (homoskedastic volatility). In Exercise 4.3 we will see how to augment a risk model in a characteristic model framework where idiosyncratic returns are heteroskedastic.

**Exercise 4.1 (Excess Returns and Factor Models):**
In the academic literature the standard factor model (4.1) models the excess returns, defined as $r_{it} - r_{ft} = \dots$ as per Section 2.1.2. On the other side, practitioners think in terms of returns, not excess returns.
*   When in portfolio management is it incorrect to reason in terms of excess returns? When is it not?
*   Show that a model of excess returns could be recast as a model of returns using factors.
*   Can you extend the modeling to incorporate sensitivities to interest rates?

---

**4.5 Applications**

**4.5.1 Performance Attribution**

What is the PnL of a portfolio in interval $[t-1, t]$?
(portfolio PnL)$_t = \mathbf{w}_t' \mathbf{r}_t$
$$ = \mathbf{w}_t' (\alpha + \mathbf{Bf}_t + \mathbf{\epsilon}_t) = \mathbf{w}_t' \alpha + \mathbf{w}_t' \mathbf{Bf}_t + \mathbf{w}_t' \mathbf{\epsilon}_t $$
The vector $\mathbf{w}_t \in \mathbb{R}^N$ represents the portfolio holdings at time $t-1$. The term $\mathbf{w}_t' \alpha$ is the characteristics of factor $j$ of each stock, weighted by the portfolio holdings. Keep in mind that the characteristics and the weights can both be negative. The term $\mathbf{w}_t' \mathbf{Bf}_t$ is the factor PnL in time interval $t$, while the term $\mathbf{w}_t' \mathbf{\epsilon}_t$ is the idiosyncratic PnL. Summing up over a time interval $[0, \dots, T]$, the PnL of a strategy is
$$ \text{PnL} = (\text{Factor PnL}) + (\text{Idiosyncratic PnL}) $$
We can also distribute the sum differently:
$$ \text{PnL} = \sum_t (\text{Factor PnL})_t + \sum_t (\text{Idiosyncratic PnL})_t $$
$$ = \sum_t \mathbf{w}_t' \mathbf{Bf}_t + \sum_t \mathbf{w}_t' (\alpha_t + \mathbf{\epsilon}_t) $$
$$ = \sum_t (\text{Factor } j \text{ PnL})_t + \sum_t (\text{Stock } i \text{ Idiosyncratic PnL})_t $$
And then, of course, one can partition factors and stocks in groups, to highlight, for example, the performance arising from style factors, from industry factors, or from a specific group of stocks.

**4.5.2 Risk Management: Forecast and Decomposition**

If we have a covariance matrix (not specifically from a factor model), the variance of a portfolio $\mathbf{w}$ is easy to compute:
$\text{var}(\mathbf{w}'\mathbf{r}) = \text{cov}(\mathbf{w}'\mathbf{r}, \mathbf{w}'\mathbf{r}) = \mathbf{w}' \Omega_r \mathbf{w}$.
We can apply the formula to a covariance matrix associated with a factor model
$$ \text{var}(\mathbf{w}'\mathbf{r}) = \mathbf{w}' (\mathbf{B} \Omega_f \mathbf{B}' + \Omega_\epsilon) \mathbf{w} $$
$$ = \mathbf{w}' \mathbf{B} \Omega_f \mathbf{B}' \mathbf{w} + \mathbf{w}' \Omega_\epsilon \mathbf{w} $$
The formula has two applications. The first one is an estimate of a portfolio’s ex-ante volatility at any point in time. This is an important piece of information for risk management, portfolio construction, and alpha research. Risk based on factor models is a common application in the decomposition of variance in factor and idiosyncratic components. Like in the attribution case, the formula is a jumping-off point. For example, a commonly quoted statistic for, a strategy is the percentage of idiosyncratic variance to total variance (also called variance ratio). The percentage of idiosyncratic variance and factor variance is the sum of two. The factor variance can be decomposed further by making factor partitions. The most detailed one has each factor being a singleton, but very common choices are [style group]/[industry group]/[country group]/[idiosyncratic group]. This information is used to manage strategies. Every partition, either of factors or of assets, induces a covariance matrix $\Omega_p = B_p \Omega_{fp} B_p'$ where $\Omega_{fp}$ is the covariance between partition group $p$ and $f$. For example, say that a portfolio has factor exposure $\mathbf{b}_p$, and we partition factors in groups $g_1, \dots, g_P$, with group $g_i$ containing a subset of factors $S_i$. Define $B_{g_i}$ as a selector matrix that selects the rows of $B$ that are in set $S_i$. Define $\Omega_{g_i}$ as the covariance matrix of the rows of PnL:
$$ \Omega = \begin{pmatrix} B_{g_1}' \Omega_f B_{g_1} & \dots & B_{g_1}' \Omega_f B_{g_P} \\ \vdots & \ddots & \vdots \\ B_{g_P}' \Omega_f B_{g_1} & \dots & B_{g_P}' \Omega_f B_{g_P} \end{pmatrix} $$
Then the total factor variance is $\mathbf{w}' \mathbf{B} \Omega_f \mathbf{B}' \mathbf{w} = \sum_{i,j=1}^P \mathbf{w}' B_{g_i}' \Omega_f B_{g_j} \mathbf{w}$.
The $g$-th group variance is $\mathbf{w}' B_{g_i}' \Omega_f B_{g_i} \mathbf{w}$.
Fraction of total variance for group $g$ is
$$ \pi_g = \frac{(\text{variance of group } i \text{ PnL}) + (\text{sum of } i \ne j \text{ covariance contributions})}{(\text{portfolio variance})} $$
$$ = \frac{\text{cov}(\mathbf{w}' B_{g_i} \mathbf{f}_{g_i}, \mathbf{w}'\mathbf{r})}{\text{var}(\mathbf{w}'\mathbf{r})} $$
$$ = \frac{(\text{beta of } i\text{-th factor group's PnL to total PnL})}{\text{var}(\mathbf{w}'\mathbf{r})} $$
So that $\sum_g \pi_g = 1$. The percentage of variance of a group $g_i$ (again, this includes single factors and single assets—perhaps the most commonly used partition) is simple the beta of returns of the group to the overall portfolio.
Marginal contribution to risk (MCR) of a group $S(i)$ is defined as
$$ m_i = \frac{\partial \text{portfolio } \sigma \text{ risk when we buy } \$1M \text{ of set } S(i)}{\partial \$1M} $$
$$ = \frac{\beta_{S(i), \text{port}} \times \sqrt{\text{var}(S(i))}}{1} = \frac{1}{\sigma_{\text{port}}} \sum_{j \in S(i)} \Omega_{ij} $$
$$ = \frac{\sigma_{S(i)}}{\sigma_{\text{port}}} (\text{correlation of } S(i) \text{ factor group PnL to total PnL}) $$
$$ = \frac{\text{vol}(S(i) \text{ PnL})}{\text{vol}(\text{PnL})} $$
Sharpe Ratio sensitivity. It is also useful to compute the sensitivity of the Sharpe Ratio to changes in volatility of a group. The total portfolio Sharpe Ratio sensitivity with respect to volatility increase of group $g$ is given by
$$ \frac{\partial E_t[\text{vol}(\text{PnL})]}{\partial \text{vol}(g \text{ PnL})} = \frac{E_t[\text{vol}(\text{PnL})]}{\text{vol}(\text{PnL})} \frac{\partial \text{vol}(\text{PnL})}{\partial \text{vol}(g \text{ PnL})} $$
$$ \text{vol}(\text{PnL}) \frac{\partial E_t[\text{PnL}]}{\partial \text{vol}(g \text{ PnL})} - E_t[\text{PnL}] \frac{\partial \text{vol}(\text{PnL})}{\partial \text{vol}(g \text{ PnL})} = \frac{m_g \text{SR}_{\text{port}} \times \text{vol}(g \text{ PnL})}{\text{vol}(\text{PnL})} $$
$$ = \frac{\text{SR}_{\text{port}} - \text{SR}_{g}}{\text{SR}_{\text{port}}} \frac{m_g}{\text{vol}(\text{PnL})} $$
The contribution to total Sharpe Ratio is positive if the Sharpe Ratio of a group exceeds the portfolio Sharpe Ratio, which is the marginal contribution to risk times the total Sharpe Ratio.

(Right Sidebar from Page 148)
does not depend on rotations. However, the single-factor risk variance is affected by rotations. Rather than being a drawback, this is a feature. We can use this flexibility to attribute risk to factors that are more meaningful (e.g., more intuitive) than others.

**FAQ 4.5: Why not use the empirical covariance matrix?**
Before treating factor model estimation, we address a preliminary question. Given a time series of returns $\mathbf{r}_t$ with population covariance matrix $\Omega_r$, its simplest estimator is the empirical covariance:
$$ \hat{\Omega}_r = \frac{1}{T} \sum_{t=1}^T \mathbf{r}_t \mathbf{r}_t' $$
or, if we denote $\tilde{\mathbf{R}} \in \mathbb{R}^{N \times T}$ the matrix of returns where $\tilde{\mathbf{R}}_{it} = r_{it}$, we can write $\hat{\Omega}_r = \frac{1}{T} \tilde{\mathbf{R}} \tilde{\mathbf{R}}'$. It is well known that if the returns are drawn from a normal multivariate distribution, $\hat{\Omega}_r$ is asymptotically consistent, and a Central Limit Theorem is available (Anderson, 1963). Why not use this as our estimate for the covariance matrix? The reason is that, even for $T \gg N$, the estimation error for volatility can be large, and portfolio optimization purposes, the covariance matrix has to be positive definite.
$$ \mathbf{w}_{LS}' \mathbf{1}_N = 0, \quad \mathbf{w}_{LS}' \tilde{\mathbf{R}} = \mathbf{0}, \quad \mathbf{w}_{LS} \ne \mathbf{0} $$
be a basis for the null space of $\tilde{\mathbf{R}}'$ (i.e., $\mathbf{w} \in \mathbb{R}^N$ s.t. $\mathbf{w}' \tilde{\mathbf{R}} = \mathbf{0}$). We can interpret these vectors as portfolios. The volatility of portfolio $\mathbf{w}_{LS}$ is
$$ \mathbf{w}_{LS}' \hat{\Omega}_r \mathbf{w}_{LS} = \frac{1}{T} \mathbf{w}_{LS}' \tilde{\mathbf{R}} \tilde{\mathbf{R}}' \mathbf{w}_{LS} = 0 $$
Six, a majority of independent portfolios has zero volatility. The situation is even worse in portfolio optimization. The solution of the mean-variance portfolio
$$ \max_{\mathbf{w}} \mathbf{w}' \alpha - \frac{1}{2\lambda} \mathbf{w}' \hat{\Omega}_r \mathbf{w} $$
is $\mathbf{w} \sim \mathcal{N}(0, \lambda^{-1} \hat{\Omega}_r^{-1})$. In this case, $\hat{\Omega}_r$ is in the null space of $\Pi_r$. The portfolio is ill-defined. Choosing an alpha close to the null space yields an arbitrarily large portfolio, and an arbitrarily large Sharpe Ratio.

**FAQ 4.4: Do model rotations affect risk decomposition?**
When we rotate a factor model, we transform the factor loadings and factor covariance matrix as $\tilde{\mathbf{B}} = \mathbf{B}C^{-1}$ and $\tilde{\Omega}_f = C \Omega_f C'$. In the rotated model, the factor exposure of portfolio $\mathbf{w}$ are
$$ \tilde{\mathbf{b}} = \tilde{\mathbf{B}}' \mathbf{w} = (C^{-1})' \mathbf{B}' \mathbf{w} = (C^{-1})' \mathbf{b} $$
and the factor risk is
$$ \tilde{\mathbf{b}}' \tilde{\Omega}_f \tilde{\mathbf{b}} = \mathbf{b}' C^{-1} C \Omega_f C' (C^{-1})' \mathbf{b} = \mathbf{b}' \Omega_f \mathbf{b} $$
The total factor variance is unchanged. The total (and single-asset) idiosyncratic variance is unchanged too, as it

---

**4.5.3 Portfolio Management**

Factor models are used in portfolio management in several ways. The first one is efficient ex ante risk management: volatility is the common language spoken by risk managers and portfolio managers, and is oftentimes generated by a factor model. The second one is the inverse of the covariance matrix, also known as precision matrix. This matrix plays a central role in portfolio optimization. As discussed in Section 4.1, a positive definite covariance matrix is a must. Factor models make this possible. A portfolio’s expected asset returns are the sum of two terms: $\alpha_i + B_{i \cdot} E[\mathbf{f}_t]$. These two terms give rise to two qualitatively different classes of expected returns. This makes sense, intuitively, since the factor-based returns come with some variability and risk, while $\alpha$ itself is systematic and comes with no risk. How to manage these sources of returns is the concern of portfolio management. Lastly, a factor model is agile: when applied to a portfolio, it produces factor exposures, risk and performance decompositions, as discussed above, explicitly. This makes the job of the portfolio managers easier, since it enables them to define action plans to monitor the strategy (during the trade), and understand (after the trade) their strategies.

**4.5.4 Alpha Research**

As volatility is the lingua franca spoken by risk managers and portfolio managers, so alpha is what a signal researcher and a portfolio manager both understand. At the cost of extreme generalization, one could say that the signal researcher cares about the risk model because it helps them identify and portfolio construction methods, so costs and tries to combine all these concerns into a profitable strategy. In reality, there is no “alpha” worth trading contained in the term $Bf$. Use the language loosely for the time being, with the goal of tightening it in coming chapters. Furthermore, common factors are not the same, although their roles are similar. Alpha research is improved by factor models in two ways. First, $Bf$ is important, and for a certain class of investors it is the only thing that matters.[...] Second, for a factor-based approach. Help separate the two sources of expected returns for a portfolio. One source is associated with having factor exposures. These returns come with the associated risk of variable factor returns. The second source is the “true alpha” of an asset, i.e., having exposure to the alpha vector.

---

**4.6 Factor Models Types**

The model we use from here on is Equation (4.1). We have taken the model for granted. But where do the data and parameters of the model come from? In the case of factor models, the answer is especially important, because the meaning attached to the various symbols matters. Practitioners use three broad approaches to identify all the parameters in the equation:

1.  Characteristic model: This is the most common approach. The input data to the model are the time series $r_t$ and $B_t$. Factor and idiosyncratic returns are estimated from these data. We define $B_{it}$ as a matrix of asset characteristics at time $t$. For example, $B_{it}$ could be the market capitalization of stock $i$ at time $t-1$. The intuition is that these characteristics are partially responsible for the stock return. I cover this in Chapter 5.
2.  Statistical model: In this model, the only premise is $r_t$ and $B_t, f_t$ and $K$ are all unobserved. We estimate these quantities from $r_t$. I cover this in Chapter 8.
3.  Macroeconomic model: In this model, the inputs are $r_t$, and $f_t$. $B_t$ and $K$ are estimated. $f_t$ usually represents a vector of macroeconomic time series.

The relevant methodological issues the modeler must address are:
1.  What are the best loss functions to evaluate a model?
2.  Once we have estimates (or promise data) about factor and idiosyncratic returns, how do we estimate the covariance matrices from cross-sectional estimates?
3.  What is the best approach within each framework?

---

**4.7 Appendix**

**4.7.1 Linear Regression**

Linear models are by far the most widespread class of models in statistics. There are entire books that treat the subject from the usual angle of the linear model. Here, we could spend pages that are much more useful if spent on their own. We could have on planet Earth may have completely different interpretations of them. In order to have some common ground, we will describe some less well-known aspects which need to be stated explicitly. Our setting is as follows. We are given a pair $(X, Y)$, where $X \in \mathbb{R}^p$ is a random vector of predictors and $Y \in \mathbb{R}$ is a random variable. We assume a general dependency, knowing the value of a realization $x$ of $X$ tells us something about the values of $Y$ and this makes the problem inherently interesting. Say that we want to provide a forecast $g(x)$ which is close to $Y$. One way to such forecast is to try to minimize the mean squared error of the forecast. The mean squared error, or loss, of the choice of the loss is the quadratic loss. It is non-negative; it is symmetric; it is differentiable; and it penalizes more for large errors. The problem we face is
(4.8)
$$ \min_g E[(Y - g(X))^2] $$
One basic result in statistics[15] and in control theory is that, if $E[Y^2] < \infty$, the function that minimizes this expectation is the conditional expectation of $Y$ given $X$. We use the notation $E[Y|X]$.
It follows that
$$ E[Y|X] = E[Y - g(X) + g(X)|X] = E[Y-g(X)|X] + E[g(X)|X] = 0 $$
Then we use the following chain:
$$ E[(Y-g(X))^2] = E[E[(Y-g(X))^2|X]] $$
$$ = E[E[(Y-E[Y|X] + E[Y|X] - g(X))^2|X]] $$
$$ = E[E[(Y-E[Y|X])^2|X] + E[(E[Y|X]-g(X))^2|X] + 2E[(Y-E[Y|X])(E[Y|X]-g(X))|X]] $$
$$ = E[(Y-E[Y|X])^2] + E[(E[Y|X]-g(X))^2] $$
$$ \ge E[(Y-E[Y|X])^2] $$
The equality holds only if $g(X) = E[Y|X]$. The term $E[Y^2]$ is finite, because
$E[Y^2] < 2E[Y^2] + 2E[g(X)^2] < \infty$.
$E[Y^2] < 2E[Y^2] + 2E[E[Y|X]^2] < \infty$.
(Jensen)
In applications we work with samples $(x_i, y_i)$ and we choose a functional form for $g = g(X, \theta)$, where $\theta$ is a finite- or infinite-dimensional vector of parameters. We then minimize the empirical squared loss
$$ n^{-1} \sum_{i=1}^n (y_i - g(x_i, \theta))^2 $$
The simplest form of $g$ is linear: $g(X, \theta) = X'\beta$. In matrix form, Equation (4.8) becomes
$$ \min_\beta (Y - X\beta)'(Y - X\beta) $$
where $Y \in \mathbb{R}^n, X \in \mathbb{R}^{n \times p}$. The integers $n$ and $m$ denote the number of observations and the number of features, respectively. We want to estimate the parameters $\beta$, leading to estimates for $X\beta$. We then minimize the empirical loss
$$ (Y - X\beta)'(Y - X\beta) $$
which is equal to the unweighted sum of squared errors (Ordinary Least Squares, OLS).
$$ \sum_{i=1}^n (y_i - x_i'\beta)^2 $$
A different route arrives at the same problem. It is to posit that the true model is $Y = X\beta + \epsilon$, where $\epsilon \sim N(0, \sigma^2 I_n)$. If we set the loss to be $-\log P(Y|X, \beta)$ and since we know the distribution of $\epsilon$, we can associate to $Y$ a choice of $\beta$ a likelihood $P(Y|X, \beta)$. If we choose the parameters to maximize the likelihood, we end up solving the same problem as Equation (4.8). The choice of maximizing the likelihood is called the Maximum Likelihood Principle (MLE).
Finally, there is a geometrical interpretation for the regression problem. You can interpret the set
$$ S := \{X\beta, \beta \in \mathbb{R}^m\} $$
as a subspace of $\mathbb{R}^n$. The columns of $X$ are a (generally non-orthonormal) basis of the subspace. We are looking for the point $X\hat{\beta}$ in the subspace $S$ that is closest to $Y$. This is the definition of orthogonal projection of $Y$ on $S$. The problem is a linear operator. The minimum distance $Y - X\hat{\beta}$ is attained at
$$ \hat{\beta} = (X'X)^{-1}X'Y $$
and the estimates $Y$ are $E[Y|X]$:
(4.9)
$$ \hat{Y} = X\hat{\beta} $$
$$ = X(X'X)^{-1}X'Y $$
The matrix
$$ H = X(X'X)^{-1}X' $$
is called the hat matrix or projection matrix. The estimated residuals are
$$ \hat{\epsilon} = (I-H)Y $$
Intuitively, the optimal estimates should not change if we change the basis of the subspace. To see this, choose a new basis $XQ$, where $Q \in \mathbb{R}^{m \times m}$ is non-singular. The new transformed set of predictors spans the same subspace as $X$. Then
(4.10)
$$ \hat{Y} = XQ((XQ)'XQ)^{-1}(XQ)'Y $$
$$ = XQ(Q'X'XQ)^{-1}Q'X'Y $$
$$ = XQQ^{-1}(X'X)^{-1}(Q')^{-1}Q'X'Y $$
$$ = X(X'X)^{-1}X'Y $$
hence $\hat{Y}$ is independent of base representation.
Another property of the estimate $\hat{Y}$ is that, if we iterate the estimation process on the estimate $\hat{Y}$, we obtain again $\hat{Y}$. This also has a geometric interpretation. Once a point has been projected on a hyperplane, the projection of the projection is unchanged. In algebraic terms,
$$ H \hat{Y} = H^2 Y = HY $$
Here is another facet of linear regression tying geometric and algebraic interpretations of linear regression. Decompose $X$ using the SVD $X = U \Lambda V'$. $U$ is an orthonormal basis for the column subspace of $X$. Then
$$ \hat{Y} = U \Lambda V' (V \Lambda U' U \Lambda V')^{-1} V \Lambda U' Y $$
$$ = U U' Y $$
So $\hat{Y}$ is projected on the column space of $U$.
Replace Equation (4.9) in the beta estimation formula (4.10) to obtain
$$ \hat{\beta} = (X'X)^{-1}X' (X\beta + \epsilon) $$
$$ = \beta + (X'X)^{-1}X'\epsilon $$
The estimate of beta is unbiased, because
$$ E[(X'X)^{-1}X'\epsilon] = 0 $$
, and the covariance matrix of $\hat{\beta}$ is
(4.11)
$$ \text{var}(\hat{\beta}) = \sigma^2 (X'X)^{-1} $$
Similarly,
$$ \text{var}(\hat{Y}) = \sigma^2 X(X'X)^{-1}X' = \sigma^2 H $$
We can write these formulas using the SVD.
(4.12)
$$ \text{var}(\hat{\beta}) = \sigma^2 V \Lambda^{-2} V' $$
(4.13)
$$ \text{var}(\hat{Y}) = \sigma^2 U U' $$
The variance of the estimates $\text{var}(\hat{\beta})$ becomes larger as the columns of $X$ become more collinear. In our interpretation of the matrix $X$, this occurs when we include factors that overlap heavily with pre-existing ones.
The formulas that extend directly to the case of heteroskedastic noise. In this case we assume that $\epsilon \sim N(0, \Omega_\epsilon)$, where $\Omega_\epsilon$ is a positive-definite matrix. The estimate for $\beta$ can be derived directly from the previous formulas, by “whitening” the data, i.e., by left-multiplying $Y$ and $X$ by $\Omega_\epsilon^{-1/2}$.
$$ \Omega_\epsilon^{-1/2} Y = \Omega_\epsilon^{-1/2} X \beta + \Omega_\epsilon^{-1/2} \epsilon $$
The noise $\Omega_\epsilon^{-1/2} \epsilon$ is distributed according to a standard normal (exercise), so that the previous formulas hold. And we apply the OLS results to obtain the Weighted Least Squares (WLS) formulas:
(4.14)
$$ \hat{\beta} = (X' \Omega_\epsilon^{-1} X)^{-1} X' \Omega_\epsilon^{-1} Y $$
(4.15)
$$ \text{var}(\hat{\beta}) = (X' \Omega_\epsilon^{-1} X)^{-1} $$

---

(4.16)
$$ \mathbf{y} = X(X'\Omega_\epsilon^{-1}X)^{-1}X'\Omega_\epsilon^{-1}\mathbf{y} $$

**Exercise 4.2:**
If a matrix $X \in \mathbb{R}^{n \times m}$ has near collinear columns, then there is a unit-norm vector $\mathbf{u}$ such that $\|X\mathbf{u}\|^2 < \delta$, for some small positive $\delta$.
1.  Show that $X'X \mathbf{u} \approx \mathbf{0}$.
2.  Let $\lambda_N$ be the eigenvalue of $X'X$. Show that $\min_\mathbf{u} \lambda_N \le \delta$.
From this, show that
$$ \sum_i \text{var}(\hat{\beta}_i)^2 \ge \max_i \lambda_i^{-2} \ge 1/\delta^2 = \|X\mathbf{u}\|^{-2} $$

**4.7.2 Linear Regression Decomposition**

Split Equation (4.8) into two parts:
(4.17)
$$ \mathbf{y} = [X_1 \quad X_2] \begin{bmatrix} \beta_1 \\ \beta_2 \end{bmatrix} + \epsilon $$
where we have partitioned the predictors $X$ into two blocks. Equation (4.17) can be rewritten using block inversion for $X'X = \begin{bmatrix} X_1'X_1 & X_1'X_2 \\ X_2'X_1 & X_2'X_2 \end{bmatrix}$, and the formulas for the block inverse which involve the Schur complement of $X_1'X_1$ in $X'X$. The estimate $\hat{\beta}_2$ can be estimated by a two-stage regression. First, regress the columns of $X_2$ on those of $X_1$: $\tilde{X}_2 = X_2 - X_1(X_1'X_1)^{-1}X_1'X_2$.
The matrix $\tilde{X}_2$ contains the components of the column vectors of $X_2$ that are orthogonal to the columns of $X_1$. We say that $\tilde{X}_2$ is the projection on the orthogonal complement of the subspace spanned by $X_1$. The subspace spanned by $(X_1, \tilde{X}_2)$ is the same as the subspace spanned by $(X_1, X_2)$ (if you do not see it, prove it). Therefore the estimates $\hat{\mathbf{y}}$ and $\hat{\beta} = [\hat{\beta}_1', \hat{\beta}_2']'$ are unchanged. Second, regress $\mathbf{y}$ on $\tilde{X}_2$:
$$ \mathbf{y} = \tilde{X}_2 \tilde{\beta}_2 + \eta $$
It can be proven (see, e.g., Hansen (2007), Ch. 2) or prove it yourself as an exercise, that the least-squares coefficient of this regression is the same as $\hat{\beta}_2$.

**4.7.3 The Frisch-Waugh-Lovell Theorem**

Let us continue along the line of reasoning of the previous section, where we characterize groups of predictors. In Equation (4.17) we did above, we saw that the estimate of $\beta_2$ is unchanged if we replace $X_2$ by $\tilde{X}_2$, the component orthogonal to the columns of $X_1$, and used the resulting matrix $\tilde{X}_2$.
However, this transformation enables us to perform regressions in consecutive stages, where each stage solves a stand-alone linear estimation problem. This insight is formalized in the following theorem.

**Theorem 4.1 (Frisch-Waugh-Lovell):**
Denote the reference model
(4.18)
$$ \mathbf{y} = X_1 \beta_1 + X_2 \beta_2 + \epsilon $$
whose estimated parameters are $\hat{\beta}_1, \hat{\beta}_2$.
Estimate the system in stages. The first-stage model is
(4.19) $Y = X_1 \gamma_1 + \eta_1$
from which we get estimates $\hat{\gamma}_1, \hat{\eta}_1$.
The second stage model uses $\hat{\eta}_1$ from the first stage.
(4.20) $\hat{\eta}_1 = X_2 \gamma_2 + \eta_2$
from which we get estimate $\hat{\gamma}_2, \hat{\eta}_2$. The following identities hold:
$$ \hat{\gamma}_1 = \hat{\beta}_1 $$
$$ \hat{\gamma}_2 = \hat{\beta}_2 $$
$$ \hat{\eta}_2 = \hat{\epsilon} $$
Proof:
We use the hat matrix of $X_1$:
$$ H_1 = X_1(X_1'X_1)^{-1}X_1' $$
The operator is a projection, i.e., $H_1^2 = H_1$ and
$$ (I_n - H_1) X_1 = \mathbf{0} $$
First, we characterize the solutions for $\hat{\beta}_1, \hat{\beta}_2$. We use the property that
$$ X_1'(I_n - H_1) X_2 = X_1'(I_n - H_1) X_2 - X_1'(X_1'X_1)^{-1}X_1'X_1 X_2 = \mathbf{0} $$
$$ \mathbf{y} = [X_1 \quad X_2] \begin{bmatrix} \beta_1 \\ \beta_2 \end{bmatrix} + \epsilon $$
$$ \begin{bmatrix} \hat{\beta}_1 \\ \hat{\beta}_2 \end{bmatrix} = \begin{bmatrix} (X_1'X_1)^{-1} & -(X_1'X_1)^{-1}X_1'X_2(X_2'M_1X_2)^{-1} \\ -(X_2'M_1X_2)^{-1}X_2'X_1(X_1'X_1)^{-1} & (X_2'M_1X_2)^{-1} \end{bmatrix} \begin{bmatrix} X_1' \\ X_2' \end{bmatrix} \mathbf{y} $$
$$ \hat{\beta}_1 = (X_1'X_1)^{-1}X_1'\mathbf{y} - (X_1'X_1)^{-1}X_1'X_2 \hat{\beta}_2 $$
$$ \hat{\beta}_2 = (X_2'M_1X_2)^{-1}X_2'M_1\mathbf{y} $$
Now, let us write the outputs $\hat{\gamma}_1, \hat{\eta}_1$ of the first stage
$$ \hat{\eta}_1 = (I_n - H_1)\mathbf{y} $$
$$ \hat{\gamma}_2 = (X_2'(I_n - H_1)X_2)^{-1}X_2'(I_n - H_1)\mathbf{y} $$
Those are used to generate $\hat{\gamma}_2, \hat{\eta}_2$. We show that they are identical to $\hat{\beta}_1, \hat{\beta}_2$.
$$ \hat{\gamma}_1 = (X_1'X_1)^{-1}X_1'\mathbf{y} $$
$$ \hat{\beta}_1 = (X_1'X_1)^{-1}X_1'(\mathbf{y} - X_2 \hat{\beta}_2) $$
$$ \hat{\eta}_2 = (I_n - H_2)(I_n - H_1)\mathbf{y} = (I_n - H_1 - H_2 + H_2H_1)\mathbf{y} $$
$$ = \hat{\beta}_2 $$
The equality of $\hat{\eta}_2$ and $\hat{\epsilon}$ follows from Equations (4.19)-(4.20).
We close the section with two remarks.
1.  In Section 4.7.2 we showed that Equation (4.20) yields the same estimate as the regression on the total returns, i.e., that the coefficient estimate $\hat{\beta}_2$ is the same as $\hat{\gamma}_2$. We also saw that, after removing the independent variable, we have the option of regressing directly on the total return. However, the estimated residuals $\hat{\eta}_1$ and $\hat{\eta}_2$ will be different.
2.  The formulas above hold for the case of identical idiosyncratic volatilities. For the general case, the formula for $\hat{X}_2$ becomes
    $$ \tilde{X}_2 = (I_n - X_1(X_1'\Omega_\epsilon^{-1}X_1)^{-1}X_1'\Omega_\epsilon^{-1})X_2 $$
    Proving this is left as an exercise (hint: pre-multiply by $\Omega_\epsilon^{-1/2}$ above, use the results above, and transform back).

**Procedure 4.1: Stagewise linear regression**
1.  Estimate the model
    (4.21) $y = X_1 \beta_1 + X_2 \beta_2 + \epsilon$
    to obtain estimates $\hat{\beta}_1$ and $\hat{\beta}_2$.
2.  Regress the columns of $X_2$ on $X_1$ and take the residuals of each regression. Define $\tilde{X}_2$ as a matrix whose $j$-th column is the residual vector of $X_{2j}$ on $X_1$.
3.  Estimate the model $y = \tilde{X}_2 \tilde{\beta}_2 + \tilde{\epsilon}$ to obtain estimates $\tilde{\beta}_2$ and $\tilde{\epsilon}$.

**4.7.4 The Singular Value Decomposition**

The Singular Value Decomposition (SVD) is a fundamental factorization in numerical linear algebra. It powers many numerical computations, as Golub and Van Loan (2012) beautifully explain. In addition, it is extremely insightful in theoretical analysis. Much theoretical research. Since it is not always covered in linear algebra courses, this Appendix provides a concise review of the SVD and its main geometric intuitions, see Trefethen and Bau (1997) and Johnson (2007). Strang (2019) and the aforementioned classic book by Golub and Van Loan.
We start by recalling a basic fact of algebra. We are given a square matrix $A$ that is symmetric and positive semidefinite ($A \in \mathbb{R}^{n \times n}$, $A=A'$, $x'Ax \ge 0$ for all $x \in \mathbb{R}^n$). Then, the $n$ eigenvalues and eigenvector of $A$, $(\lambda_i, v_i)$, where $Av_i = \lambda_i v_i$, are unit-norm vectors. Then, the eigenvalues are real, positive, and the eigenvectors are orthonormal, i.e., $v_i'v_j = \delta_{ij}$. What can be said about general rectangular matrices $A \in \mathbb{R}^{m \times n}$? It is possible to generalize the notion of eigenvalues and eigenvectors to this case. The SVD states that there exist two orthonormal bases $\{u_i\}_{i=1}^m \subset \mathbb{R}^m$ and $\{v_j\}_{j=1}^n \subset \mathbb{R}^n$ that span the column space and the row space of $A$, respectively. The vectors $u_i$ are the left singular vectors, the vectors $v_j$ are the right singular vectors. Let $r = \text{rank}(A)$. The image subspace of $A$ has dimension $r$ if there are $r$ independent vectors $u_i$. The kernel subspace has dimension $n-r$ if there are $n-r$ independent vectors $v_j$, such that $Av_j = 0$. We partition $V = [V_1 \quad V_2]$, where $V_1 \in \mathbb{R}^{n \times r}$ and $V_2 \in \mathbb{R}^{n \times (n-r)}$.
(4.22)
$$ A v_i = s_i u_i \quad 1 \le i \le r $$
$$ A v_i = 0 \quad r < i \le m $$
We can write these equations in matrix form:
$$ A [v_1 \quad \dots \quad v_m] = [u_1 \quad \dots \quad u_r \quad 0 \quad \dots \quad 0] \begin{pmatrix} s_1 & & & & \\ & \ddots & & & \\ & & s_r & & \\ & & & 0 & \\ & & & & \ddots \end{pmatrix} $$
Here, in addition to the vectors $u_1, \dots, u_m$, we have completed this orthonormal basis with $u_{r+1}, \dots, u_m$, so that it spans $\mathbb{R}^m$. In compact form, Equation (4.22) can be written as $AV = U\Sigma$, where $\Sigma \in \mathbb{R}^{m \times n}$ is a diagonal matrix, with $s_i \ge 0$ on its main diagonal. Finally, we rewrite the equation after right-multiplying by $V^T$ as
(4.23)
$$ A = U \Sigma V^T $$
We show the decomposition visually in Figure 4.5.

[Image: Figure 4.5: A Singular Value Decomposition, $A = U \Sigma V^T$. Shows matrices U, Sigma, V^T.]

We prove Equations (4.22) by noting that $A^T A$ is a positive semidefinite matrix of rank $r$, so that there are $r$ pairs $(v_i, s_i^2)$ satisfying $(A^T A)v_i = s_i^2 v_i$. Define $u_i := s_i^{-1} A v_i$. These satisfy Equation (4.22). We prove the $u_i$ are orthonormal.
(4.24)
$$ u_i' u_j = s_i^{-1} v_i' A^T A v_j s_j^{-1} = s_i^{-1} v_i' s_j^2 v_j s_j^{-1} = \frac{s_j}{s_i} v_i' v_j = \delta_{ij} $$
Because $v_1, \dots, v_n$ are orthonormal. Now we complete the basis in $\mathbb{R}^m$ by adding orthonormal vectors $u_{r+1}, \dots, u_m$. Then $U$ and $V$ are orthonormal matrices, and $\Sigma$ is a diagonal matrix. This makes a connection to the SVD. A few observations (among many) on the SVD:
1.  If all the singular values are distinct, the first $r$ columns of $U$ and $V$ are uniquely determined. However, they are not in the case of identical singular values.
2.  The SVD of a symmetric positive definite matrix $A$, and in particular for the covariance matrix, can easily be defined from the SVD.
    $$ A^T = (U \Sigma V^T)^T = V \Sigma^T U^T = V \Sigma U^T $$
    The condition $A=A^T$ implies that this definition meets the requirement of representation. For the specific case of the square root, one can show that $A^{1/2} A^{1/2} = A$.
    Equation (4.23) can be rewritten as:
    $$ A = \sum_{i=1}^r s_i u_i v_i^T $$
    The SVD decomposes a matrix into a sum of rank-one matrices.
3.  For all $k \le r$, $A_k = \sum_{i=1}^k s_i u_i v_i^T$.
4.  $A A^T u_i = s_i^2 u_i$ and $A^T A v_i = s_i^2 v_i$.
    In other terms, $A A^T$ and $A^T A$ have the same eigenvalues.
5.  The SVD decomposes the operations on an element in $\mathbb{R}^n$ into a rotation, a rescaling of the axes (turning a ball into an ellipsoid), followed by another rotation. The net result is that any operator $A$ maps a point on a ball into a point on a rotated ellipsoid. Figure 4.6 illustrates the steps of the SVD.

[Image: Figure 4.6: Singular Value Decomposition as a sequence of steps: rotation, scaling, rotation. Shows a circle transformed into ellipses through these steps.]

---

**4.8 Exercises**

**Exercise 4.3 (Portfolio Covariance):**
Generalize this result. Let $\mathbf{f}$ be a random vector taking values in $\mathbb{R}^m$ with covariance matrix $\Omega_f$. Let $A \in \mathbb{R}^{n \times m}$. Prove that the covariance matrix of the random vector $A\mathbf{f}$ is $A \Omega_f A^T$.
Say that a random vector $\mathbf{x}$ follows a multivariate normal distribution with covariance matrix $\Omega$. Let $U$ be the Singular Value Decomposition of $\Omega = U \Lambda^{1/2} \Lambda^{1/2} U^T$ and define
$$ \Omega^{1/2} U = \begin{pmatrix} \lambda_1^{1/2} & & & \\ & \lambda_2^{1/2} & & \\ & & \ddots & \\ & & & \lambda_m^{1/2} \end{pmatrix} U^T $$
Let $\mathbf{z}$ be a Gaussian distribution with unit covariance matrix. Prove that $E[\mathbf{z}\mathbf{z}^T] = I_m$. Then covariance is $\Omega$.

**Exercise 4.4:**
Provide a counterexample in which it is not possible to center the loadings with a rotation. (Hint: Use a one-factor model.)

**Exercise 4.5:**
Find conditions under which matrix $(X_1'X_1)$ is invertible.

**The Takeaways**

1.  Factor models express asset returns as a combination of factor contributions and idiosyncratic components. Their general form is
    $$ \mathbf{r}_t = \alpha + \mathbf{Bf}_t + \mathbf{\epsilon}_t $$
2.  Interpretations of Factor Models:
    *   Graphical Model: Illustrates dependencies between assets and factors using nodes and edges.
    *   Superposition of Effects: Asset returns are a sum of weighted factor contributions.
    *   Single-Asset Product: Expected return of an asset is the inner product of the factor loadings and factor returns.
3.  Alpha Components:
    *   Alpha Spanned: Portion of alpha explained by factors, indistinguishable from expected factor returns.
    *   Alpha Orthogonal: Portion of alpha unexplained by factors; offers diversification benefits and can enhance Sharpe Ratio.
4.  Transformations of Factor Models:
    *   Rotations: Change factor representations without altering model predictions (e.g., for interpretability).
    *   Projections: Reduce the number of factors while approximating the original model.
    *   Push-Outs: Expand the model by adding new factors to capture additional structure.
5.  Applications of Factor Models:
    *   Performance Attribution: Decompose portfolio returns into factor and idiosyncratic attributions.
    *   Risk Management: Forecast and decompose portfolio volatility into systematic and idiosyncratic risk.
    *   Portfolio Construction: Optimize expected returns, factor risk exposures and portfolios.
    *   Alpha Research: Identify sources of returns and separate skill from luck by analyzing alpha components.

**Notes**

1.  [1] Factor models go back to the birth of psychometrics at the turn of the 19th century. Seminal contributions to the subject are Johnson and Wichern (2007), Rencher and Christensen (2012). The use of factor models was first introduced by Sharpe (1964, 1963a) for the one-factor case, which was extended to multiple factors by Ross (1976b). Good introductions to factor models in finance are the survey papers by Connor and Korajczyk (2010), Fama et al. (1969) or the survey by Connor et al. (2010) and Connor and Korajczyk (2010), MacKinlay (1995).
2.  [2] We also use the terms residual and specific in place of “idiosyncratic”.
3.  [3] The use of the term “style” will be clear later, when we associate it with loading styles.
4.  [4] A dummy variable taking binary values (e.g., 0 or 1) or more generally values in a finite set. We will only use binary variables.
5.  [5] Graphical models are covered in monographs (Lauritzen, 1996), books on machine learning (Bishop, 2006; Murphy, 2012), and survey papers (Meilă et al., 2007).
6.  [6] Fair Market Value (FMV) is the amount invested in the security in the reference currency (numeraire).
7.  [7] For example, for U.S. equities, the maximum annualized idiosyncratic volatility is approximately 25%.
8.  [8] We have given up on the elegant eigenvalue of the sparse matrix $\Omega_\epsilon$.
9.  [9] Economic reason in terms of “increasing” economies as a function of the tradable assets. I believe there is not much to gain from this level of abstraction, so I keep the exposition and the inequalities in finite dimensions.
10. [10] See Appendix 4.7.4 for an introduction to the SVD and the concept of rank of a matrix, which we will use throughout the book.
11. [11] We use the notation $x \in [0, 1, \dots]$.
12. [12] A marketing term used for this investment style is smart beta.
13. [13] Important: when using commercial data, always check the data specification carefully. Make sure that the time index for the loadings is $t-1$ (by convention) and that the corresponding asset returns are for the period $[t-1, t]$.
14. [14] Not a joke: as of October 2012, the Vatican has 764 citizens; Amazon lists over 1,000 books in the “Probability and Statistics” section with “regression” in their title or subject, the vast majority of them covering linear models.
15. [15] Linear regression is an inexhaustible topic. Some useful references are, in order of increasing detail, Wasserman (2004), Hastie et al. (2008), Johnson and Wichern (2007), Harrell (2015), Gelman et al. (2013), Hansen (2019).
16. [16] For a detailed discussion of the MLE, see Robert (2007).
17. [17] The minimum is unique if the rank of $X$ is $m$, i.e., if all the columns of $X$ are linearly independent. In Chapter 5 we will encounter cases of rank-deficient matrices.

```

Okay, here is the Markdown compilation for Chapter 5.

```markdown
Everand

